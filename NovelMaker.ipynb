{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9337010",
   "metadata": {},
   "source": [
    "# HuggingFace_PipeLine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0250a3",
   "metadata": {},
   "source": [
    "https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct-GGUF\n",
    "\n",
    "qwen2.5-0.5b-instruct-q4_k_m.gguf\n",
    "\n",
    "Qwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters. Qwen2.5 brings the following improvements upon Qw\n",
    "Significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\n",
    "Long-context Support up to 128K tokens and can generate up to 8K tokens.\n",
    "Multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\n",
    "\n",
    "### Ïñ∏Ïñ¥Î™®Îç∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4edf95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: llama_cpp_python 0.3.16\n",
      "Uninstalling llama_cpp_python-0.3.16:\n",
      "  Successfully uninstalled llama_cpp_python-0.3.16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping llama-cpp-python-cu121 as it is not installed.\n",
      "WARNING: Skipping llama-cpp-python-cu118 as it is not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-cpp-python\n",
      "  Downloading llama_cpp_python-0.3.16.tar.gz (50.7 MB)\n",
      "     ---------------------------------------- 0.0/50.7 MB ? eta -:--:--\n",
      "     - -------------------------------------- 1.6/50.7 MB 16.6 MB/s eta 0:00:03\n",
      "     ---- ----------------------------------- 5.2/50.7 MB 17.7 MB/s eta 0:00:03\n",
      "     ------- ------------------------------- 10.0/50.7 MB 19.4 MB/s eta 0:00:03\n",
      "     ---------- ---------------------------- 13.6/50.7 MB 19.0 MB/s eta 0:00:02\n",
      "     ------------ -------------------------- 15.7/50.7 MB 17.1 MB/s eta 0:00:03\n",
      "     ------------- ------------------------- 18.1/50.7 MB 16.3 MB/s eta 0:00:03\n",
      "     -------------- ------------------------ 18.9/50.7 MB 14.7 MB/s eta 0:00:03\n",
      "     --------------- ----------------------- 20.2/50.7 MB 12.7 MB/s eta 0:00:03\n",
      "     ---------------- ---------------------- 21.0/50.7 MB 11.7 MB/s eta 0:00:03\n",
      "     ----------------- --------------------- 22.8/50.7 MB 11.4 MB/s eta 0:00:03\n",
      "     ------------------- ------------------- 25.7/50.7 MB 11.7 MB/s eta 0:00:03\n",
      "     ---------------------- ---------------- 28.8/50.7 MB 12.0 MB/s eta 0:00:02\n",
      "     -------------------------- ------------ 34.3/50.7 MB 13.1 MB/s eta 0:00:02\n",
      "     --------------------------- ----------- 36.2/50.7 MB 12.9 MB/s eta 0:00:02\n",
      "     ---------------------------- ---------- 36.4/50.7 MB 12.1 MB/s eta 0:00:02\n",
      "     ---------------------------- ---------- 37.2/50.7 MB 11.8 MB/s eta 0:00:02\n",
      "     ----------------------------- --------- 38.8/50.7 MB 11.1 MB/s eta 0:00:02\n",
      "     ---------------------------------- ---- 44.8/50.7 MB 12.1 MB/s eta 0:00:01\n",
      "     ------------------------------------ -- 47.4/50.7 MB 12.1 MB/s eta 0:00:01\n",
      "     --------------------------------------  49.8/50.7 MB 12.1 MB/s eta 0:00:01\n",
      "     --------------------------------------  50.6/50.7 MB 12.0 MB/s eta 0:00:01\n",
      "     --------------------------------------- 50.7/50.7 MB 11.4 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\venus\\anaconda3\\envs\\hg_pl\\lib\\site-packages (from llama-cpp-python) (4.15.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\venus\\anaconda3\\envs\\hg_pl\\lib\\site-packages (from llama-cpp-python) (2.3.2)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in c:\\users\\venus\\anaconda3\\envs\\hg_pl\\lib\\site-packages (from llama-cpp-python) (5.6.3)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in c:\\users\\venus\\anaconda3\\envs\\hg_pl\\lib\\site-packages (from llama-cpp-python) (3.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\venus\\anaconda3\\envs\\hg_pl\\lib\\site-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): started\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): finished with status 'error'\n",
      "Failed to build llama-cpp-python\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  √ó Building wheel for llama-cpp-python (pyproject.toml) did not run successfully.\n",
      "  ‚îÇ exit code: 1\n",
      "  ‚ï∞‚îÄ> [20 lines of output]\n",
      "      \u001b[32m*** \u001b[1mscikit-build-core 0.11.6\u001b[0m using \u001b[34mCMake 4.1.0\u001b[39m\u001b[0m \u001b[31m(wheel)\u001b[0m\n",
      "      \u001b[32m***\u001b[0m \u001b[1mConfiguring CMake...\u001b[0m\n",
      "      2025-08-28 15:30:15,114 - scikit_build_core - WARNING - Can't find a Python library, got libdir=None, ldlibrary=None, multiarch=None, masd=None\n",
      "      loading initial cache file C:\\Users\\venus\\AppData\\Local\\Temp\\tmpf3an4j0b\\build\\CMakeInit.txt\n",
      "      -- Building for: NMake Makefiles\n",
      "      \u001b[31mCMake Error at CMakeLists.txt:3 (project):\n",
      "        Running\n",
      "      \n",
      "         'nmake' '-?'\n",
      "      \n",
      "        failed with:\n",
      "      \n",
      "         no such file or directory\n",
      "      \n",
      "      \u001b[0m\n",
      "      \u001b[0mCMake Error: CMAKE_C_COMPILER not set, after EnableLanguage\u001b[0m\n",
      "      \u001b[0mCMake Error: CMAKE_CXX_COMPILER not set, after EnableLanguage\u001b[0m\n",
      "      -- Configuring incomplete, errors occurred!\n",
      "      \u001b[31m\n",
      "      \u001b[1m***\u001b[0m \u001b[31mCMake configuration failed\u001b[0m\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for llama-cpp-python\n",
      "ERROR: Failed to build installable wheels for some pyproject.toml based projects (llama-cpp-python)\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y llama-cpp-python llama-cpp-python-cu121 llama-cpp-python-cu118\n",
    "!pip install llama-cpp-python\n",
    "# GTX ÏùÑ ÏúÑÌïòÏó¨... „Ö†„Ö†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b52c5cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\venus\\.cache\\huggingface\\hub\\models--Qwen--Qwen2.5-0.5B-Instruct-GGUF\\snapshots\\9217f5db79a29953eb74d5343926648285ec7e67\\qwen2.5-0.5b-instruct-q4_k_m.gguf\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "model_path = hf_hub_download(repo_id=\"Qwen/Qwen2.5-0.5B-Instruct-GGUF\", filename = \"qwen2.5-0.5b-instruct-q4_k_m.gguf\") # Îã§Ïö¥Î°úÎìú ÌååÏùºÏùò local path Ï†ÄÏû•\n",
    "print(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75ff432d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "from huggingface_hub import hf_hub_download\n",
    "import os\n",
    "\n",
    "# Î™®Îç∏ Îã§Ïö¥Î°úÎìú & Î°úÎìú\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"Qwen/Qwen2.5-0.5B-Instruct-GGUF\",\n",
    "    filename=\"qwen2.5-0.5b-instruct-q4_k_m.gguf\"\n",
    ")\n",
    "\n",
    "llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_ctx=2048,     \n",
    "    n_threads= max(4, os.cpu_count() - 2),\n",
    "    n_batch=256,    \n",
    "    n_gpu_layers=0,  # CPU Ï†ÑÏö© GTX 1050...\n",
    "    verbose=False      # ‚Üê Î°úÍ∑∏ Ïà®ÍπÄ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "60e29efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====== üìñ ÌòÑÏû¨ÍπåÏßÄÏùò Ïù¥ÏïºÍ∏∞ (ÌÜµÌï© Ï∂úÎ†•) ======\n",
      "\n",
      "[1] ÏÉõÎ≥ÑÏù¥ÎùºÎäî Í≥†ÏñëÏù¥Îäî ÎßàÎ≤ïÏÇ¨Í∞Ä ÌÇ§Ïö∞Îäî Í≥†ÏñëÏù¥ÏòÄÎã§. Ïñ¥ÎäêÎÇ† ÎßàÎ≤ïÏÇ¨Í∞Ä ÎßêÌñàÎã§. ÏÉõÎ≥ÑÏïÑ Ïû†ÏãúÎèôÏïà ÎÇòÎäî ÏßëÏùÑ Îñ†ÎÇòÍ≤å ÎêòÏóàÎã®Îã§. Í∑∏ÎèôÏïà ÏßëÏùÑ ÏßÄÏºúÏ§Ñ Ïàò ÏûàÍ≤†Îãà? ÏÉõÎ≥ÑÏù¥Îäî ÏïÑÎ¨¥Í≤ÉÎèÑ Î™®Î•∏Ï±Ñ ÏïºÏòπÏù¥ÎùºÍ≥† ÎåÄÎãµÌñàÎã§. Í∑∏ÎïåÍπåÏßÄ Î™®Î•¥Í≥† ÏûàÏóàÎã§. ÏÉõÎ≥ÑÏù¥Ïùò Ï£ºÏù∏Ïù¥ ÎèåÏïÑÏò§ÏßÄ ÏïäÏùÑ Í≤ÉÏù¥ÎûÄ ÏÇ¨Ïã§ÏùÑ... ÏÉõÎ≥ÑÏù¥Îäî ÌïòÎ£®Í∞Ä ÏßÄÎÇòÍ≥† Ïù¥ÌãÄÏù¥ ÏßÄÎÇòÍ≥† Ïñ∏Ï††Í∞Ñ Ï£ºÏù∏Ïù¥ ÎèåÏïÑÏò¨ Í≤ÉÏù¥Îùº ÎØøÏúºÎ©∞ Íº¨Î∞ï 1ÎÖÑÏùÑ Í∏∞Îã§Î†∏Îã§. Í∑∏Îü¨ÎÇò Ï£ºÏù∏ÏùÄ ÎèåÏïÑÏò§ÏßÄ ÏïäÏïòÎã§. Ï£ºÏù∏Ïù¥ ÏûêÏã†ÏùÑ Î≤ÑÎ†∏ÏúºÎ¶¨Îùº ÏÉùÍ∞ÅÌïòÏßÄ ÏïäÎäî ÏàúÏàòÌïú Í≥†ÏñëÏù¥Ïù∏ ÏÉõÎ≥ÑÏùÄ Ï£ºÏù∏Ïù¥ Í∏∏ÏùÑ ÏûÉÏñ¥Î≤ÑÎ†∏ÏúºÎ¶¨Îùº ÏÉùÍ∞ÅÌñàÎã§. Í∑∏ÎûòÏÑú ÏÉõÎ≥ÑÏù¥Îäî ÏßëÏùÑ ÎÇòÏÑ∞Îã§. Ï£ºÏù∏ÏùÑ Ï∞æÍ∏∞ÏúÑÌï¥, Ï£ºÏù∏ÏùÑ Ï∞æÏïÑÏÑú ÏßëÏúºÎ°ú Ìï®Íªò ÎèåÏïÑÏò§Í∏∞ ÏúÑÌï¥ ÎßêÏù¥Îã§. ÏÉõÎ≥ÑÏù¥Í∞Ä ÏßëÏùÑ ÎÇòÏÑúÍ≥† Ï≤òÏùåÏúºÎ°ú Î∞©Î¨∏Ìïú Í≥≥ÏùÄ Ïñ∏ÎçïÏïÑÎûò ÎßàÏùÑÏù¥ÏóàÎã§. Ïñ∏ÎçïÏïÑÎûò ÎßàÏùÑÏùÄ ÏÇ¨ÎûåÎì§Ïù¥ Î¨¥Î¶¨ÏßÄÏñ¥ ÏÇ¥Í≥† ÏûàÏóàÎäîÎç∞ Í∑∏Í≥≥Ïóê ÏÇ¨Îäî ÏÇ¨ÎûåÎì§ÏùÄ Í≥†ÏñëÏù¥Î•º Ïã´Ïñ¥ÌïòÍ∏∞ ÎïåÎ¨∏Ïóê Ï£ºÏù∏Ïù¥ Í∞ÄÏßÄÎßêÎùºÍ≥† ÎãπÎ∂ÄÌñàÏóàÎã§. Í∑∏Îü¨ÎÇò ÏÉõÎ≥ÑÏù¥Îäî Ï£ºÏù∏ÏùÑ Ï∞æÍ∏∞ ÏúÑÌï¥ ÏÇ¨ÎûåÏù¥ ÏÇ¨Îäî ÎßàÏùÑÎ°ú Í∞ÄÏïºÎßå ÌïúÎã§Í≥† ÏÉùÍ∞ÅÌñàÎã§. ÎßàÏùÑÏóê ÎèÑÏ∞©ÌïòÏûê ÏÇ¨ÎûåÎì§ÏùÄ ÏÉõÎ≥ÑÏù¥Î•º Î∞îÎùºÎ≥¥Î©∞ Í∏∞Î∂ÑÎÇòÏÅòÎã§Îäî ÌëúÏ†ïÏùÑ ÏßÄÏóàÎã§. ÏàúÏàòÌïú ÏÉõÎ≥ÑÏù¥Îäî Í¥úÏ∞ÆÏúºÎ¶¨Îùº ÏÉùÍ∞ÅÌïòÎ©∞ Í±∏Ïñ¥ÎÇòÍ∞îÎã§. Í∑∏Îïå ÎßàÏùÑÏùò Ìïú ÏÇ¨ÎûåÏù¥ Î©ÄÎ¶¨ÏÑú ÏÉõÎ≥ÑÏù¥ÏóêÍ≤å ÎèåÏùÑ ÎçòÏ°åÍ≥†, ÏïÑÎ¨¥Í≤ÉÎèÑ Î™®Î•¥Îäî ÏÉõÎ≥ÑÏù¥Í∞Ä ÎèåÏùÑ ÎßûÏúºÎ†§Îäî ÏàúÍ∞Ñ...! ÏòÜÏóêÏÑú ÎàÑÍµ∞Í∞Ä ÎèåÏùÑ ÎßâÏïÑÏ£ºÏóàÎã§. ÏÉõÎ≥ÑÏù¥Í∞Ä ÎÜÄÎûÄ ÎààÏúºÎ°ú Ïò¨Î†§Îã§Î≥¥Ïûê Í∑∏Í≥≥Ïóî Îπ®Í∞ÑÎ®∏Î¶¨Î•º Í∞ÄÏßÑ Ìïú ÏÜåÎÖÄÍ∞Ä ÏûàÏóàÎã§.\n",
      "\n",
      "[2] [Ï†úÏïΩ] Ìïú Î¨∏Îã®Îßå, Ï§ÑÎ∞îÍøà 0~1Ìöå, Î∞òÎ≥µ ÌëúÌòÑ ÏûêÏ†ú\n",
      "\n",
      "[3] ÎÇòÎäî Í∑∏ ÏÜåÎÖÄÏóêÍ≤å ÎßêÌñàÎã§. ÎãπÏã†Ïù¥ Í≥†ÏñëÏù¥Î•º Ïã´Ïñ¥ÌïòÎäî Ïù¥Ïú†Í∞Ä Î¨¥ÏóáÏùºÍπå? ÎãπÏã†ÏùÄ Í≥†ÏñëÏù¥Îäî Ïã¨ÏßÄÏñ¥ Í∞ÄÏ°±Í≥ºÎèÑ ÏÇ¨ÎûëÌïòÏßÄ ÏïäÎäî Î∂ÑÎì§Ïù¥ ÏûàÎã§Îäî Í≤ÉÏùÑ ÏïåÍ≥† ÏûàÎäî Í±∞Îã§. ÌïòÏßÄÎßå Ïù¥Îì§ÏùÄ Í≥†ÏñëÏù¥Î•º ÏÇ¨ÎûëÌïòÎäî ÏÇ¨ÎûåÎì§Í≥º Í∞ôÏùÄ ÏÇ¨ÎûåÎì§ÏùÑ ÏúÑÌï¥ Ïã∏Ïö∞Îäî Í±∞Ïïº. Í≥†ÏñëÏù¥Ïùò ÏÜêÏùÑ Ïû°Îäî Í≤ÉÏùÄ Í∑∏Ï†Ä Í≥†ÏñëÏù¥Í∞Ä Ïã´Ïñ¥ÌïòÎäî ÏÇ¨ÎûåÎì§ÏóêÍ≤åÎßå Í∞ÄÎä•ÌïòÎã§Îäî Í±∞Ïïº. Í≥†ÏñëÏù¥Îäî Ïã¨ÏßÄÏñ¥ Í∞ÄÏ°±Í≥ºÎèÑ ÏÇ¨ÎûëÌïòÏßÄ ÏïäÎäî ÏÇ¨ÎûåÎì§ÏùÑ ÏúÑÌï¥ Ïã∏Ïö∞Îäî Í±∞Ïïº. Í∑∏Îì§ÏùÄ Í≥†ÏñëÏù¥Î•º ÏÇ¨ÎûëÌïòÎäî ÏÇ¨ÎûåÎì§Í≥º Í∞ôÏùÄ ÏÇ¨ÎûåÎì§ÏùÑ ÏúÑÌï¥\n",
      "\n",
      "\n",
      "‚úÖ Ï†ÄÏû• ÏôÑÎ£å: C:\\NLP\\practing\\story.json\n",
      "‚úÖ story.txt Ï†ÄÏû• ÏôÑÎ£å\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "from huggingface_hub import hf_hub_download\n",
    "import os, json\n",
    "from pathlib import Path\n",
    "\n",
    "SAVE_PATH = Path(\"story.json\")\n",
    "\n",
    "def load_story():\n",
    "    if SAVE_PATH.exists():\n",
    "        with open(SAVE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "    return []\n",
    "\n",
    "def save_txt(story, txt_path=\"story.txt\"):\n",
    "    with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"ÏÜåÏÑ§ Ï¥àÏïà\\n\\n\")\n",
    "        for i, p in enumerate(story, 1):\n",
    "            f.write(f\"[{i}] {p}\\n\\n\")\n",
    "\n",
    "def save_story(story):\n",
    "    with open(SAVE_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(story, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def continue_paragraph(paragraphs, genre=\"ÌåêÌÉÄÏßÄ ÎØ∏Ïä§ÌÑ∞Î¶¨\", style=\"Í∞êÏÑ± Î¨òÏÇ¨\"):\n",
    "    guide = (\n",
    "        f\"[Ïó≠Ìï†] ÌïúÍµ≠Ïñ¥ Î¨∏Ìïô ÏÜåÏÑ§Í∞Ä\\n\"\n",
    "        f\"[Ïû•Î•¥] {genre}\\n\"\n",
    "        f\"[Ïä§ÌÉÄÏùº] {style} / Î¨∏Ïû•ÏùÄ ÏûêÏó∞Ïä§ÎüΩÍ≥† Ïó∞Í≤∞ÏÑ± ÏûàÍ≤å, Í≥ºÏû•Îêú Ï∂îÏÉÅ ÌëúÌòÑ Í∏àÏßÄ\\n\"\n",
    "        f\"[Ï†úÏïΩ] Ìïú Î¨∏Îã®Îßå, Ï§ÑÎ∞îÍøà 0~1Ìöå, Î∞òÎ≥µ ÌëúÌòÑ ÏûêÏ†ú\\n\"\n",
    "    )\n",
    "    sofar = \"\\n\\n\".join(paragraphs)\n",
    "    prompt = (\n",
    "        f\"{guide}\\n\"\n",
    "        f\"ÏßÄÍ∏àÍπåÏßÄÏùò Ïù¥ÏïºÍ∏∞:\\n{sofar}\\n\\n\"\n",
    "        f\"[Îã§Ïùå Î¨∏Îã®]\\n\"\n",
    "    )\n",
    "    out = llm(\n",
    "        prompt,\n",
    "        max_tokens=140,\n",
    "        temperature=0.6,\n",
    "        top_p=0.8,\n",
    "        top_k=40,\n",
    "        repeat_penalty=1.07,\n",
    "        stop=[\"\\n\\n\\n\",\"###\",\"</s>\"]\n",
    "    )\n",
    "    text = out[\"choices\"][0][\"text\"].strip()\n",
    "    return text.split(\"\\n\\n\")[0].strip()\n",
    "\n",
    "story = load_story()\n",
    "if not story:\n",
    "    start = input(\"ÏÜåÏÑ§ Ï≤´ Î¨∏Ïû•ÏùÑ ÏûÖÎ†•ÌïòÏÑ∏Ïöî: \")\n",
    "    story = [start]\n",
    "\n",
    "turns = int(input(\"Ïù¥Î≤àÏóê Î™á Î¨∏Îã®ÏùÑ Îçî Ïù¥Ïñ¥ Ïì∏ÍπåÏöî? (Ïòà: 2): \") or \"2\")\n",
    "for _ in range(turns):\n",
    "    story.append(continue_paragraph(story))\n",
    "\n",
    "print(\"\\n====== üìñ ÌòÑÏû¨ÍπåÏßÄÏùò Ïù¥ÏïºÍ∏∞ (ÌÜµÌï© Ï∂úÎ†•) ======\\n\")\n",
    "for i, para in enumerate(story, 1):\n",
    "    print(f\"[{i}] {para}\\n\")\n",
    "\n",
    "# Ï†ÄÏû•\n",
    "save_story(story)\n",
    "print(f\"\\n‚úÖ Ï†ÄÏû• ÏôÑÎ£å: {SAVE_PATH.resolve()}\")\n",
    "\n",
    "save_txt(story)\n",
    "print(\"‚úÖ story.txt Ï†ÄÏû• ÏôÑÎ£å\")\n",
    "# llm(prompt, ... )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hg_pl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
