{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9337010",
   "metadata": {},
   "source": [
    "# HuggingFace_PipeLine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0250a3",
   "metadata": {},
   "source": [
    "https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct-GGUF\n",
    "\n",
    "qwen2.5-0.5b-instruct-q4_k_m.gguf\n",
    "\n",
    "Qwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters. Qwen2.5 brings the following improvements upon Qw\n",
    "Significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\n",
    "Long-context Support up to 128K tokens and can generate up to 8K tokens.\n",
    "Multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\n",
    "\n",
    "### ì–¸ì–´ëª¨ë¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4edf95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: llama_cpp_python 0.3.16\n",
      "Uninstalling llama_cpp_python-0.3.16:\n",
      "  Successfully uninstalled llama_cpp_python-0.3.16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping llama-cpp-python-cu121 as it is not installed.\n",
      "WARNING: Skipping llama-cpp-python-cu118 as it is not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-cpp-python\n",
      "  Downloading llama_cpp_python-0.3.16.tar.gz (50.7 MB)\n",
      "     ---------------------------------------- 0.0/50.7 MB ? eta -:--:--\n",
      "     - -------------------------------------- 1.6/50.7 MB 16.6 MB/s eta 0:00:03\n",
      "     ---- ----------------------------------- 5.2/50.7 MB 17.7 MB/s eta 0:00:03\n",
      "     ------- ------------------------------- 10.0/50.7 MB 19.4 MB/s eta 0:00:03\n",
      "     ---------- ---------------------------- 13.6/50.7 MB 19.0 MB/s eta 0:00:02\n",
      "     ------------ -------------------------- 15.7/50.7 MB 17.1 MB/s eta 0:00:03\n",
      "     ------------- ------------------------- 18.1/50.7 MB 16.3 MB/s eta 0:00:03\n",
      "     -------------- ------------------------ 18.9/50.7 MB 14.7 MB/s eta 0:00:03\n",
      "     --------------- ----------------------- 20.2/50.7 MB 12.7 MB/s eta 0:00:03\n",
      "     ---------------- ---------------------- 21.0/50.7 MB 11.7 MB/s eta 0:00:03\n",
      "     ----------------- --------------------- 22.8/50.7 MB 11.4 MB/s eta 0:00:03\n",
      "     ------------------- ------------------- 25.7/50.7 MB 11.7 MB/s eta 0:00:03\n",
      "     ---------------------- ---------------- 28.8/50.7 MB 12.0 MB/s eta 0:00:02\n",
      "     -------------------------- ------------ 34.3/50.7 MB 13.1 MB/s eta 0:00:02\n",
      "     --------------------------- ----------- 36.2/50.7 MB 12.9 MB/s eta 0:00:02\n",
      "     ---------------------------- ---------- 36.4/50.7 MB 12.1 MB/s eta 0:00:02\n",
      "     ---------------------------- ---------- 37.2/50.7 MB 11.8 MB/s eta 0:00:02\n",
      "     ----------------------------- --------- 38.8/50.7 MB 11.1 MB/s eta 0:00:02\n",
      "     ---------------------------------- ---- 44.8/50.7 MB 12.1 MB/s eta 0:00:01\n",
      "     ------------------------------------ -- 47.4/50.7 MB 12.1 MB/s eta 0:00:01\n",
      "     --------------------------------------  49.8/50.7 MB 12.1 MB/s eta 0:00:01\n",
      "     --------------------------------------  50.6/50.7 MB 12.0 MB/s eta 0:00:01\n",
      "     --------------------------------------- 50.7/50.7 MB 11.4 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\venus\\anaconda3\\envs\\hg_pl\\lib\\site-packages (from llama-cpp-python) (4.15.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\venus\\anaconda3\\envs\\hg_pl\\lib\\site-packages (from llama-cpp-python) (2.3.2)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in c:\\users\\venus\\anaconda3\\envs\\hg_pl\\lib\\site-packages (from llama-cpp-python) (5.6.3)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in c:\\users\\venus\\anaconda3\\envs\\hg_pl\\lib\\site-packages (from llama-cpp-python) (3.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\venus\\anaconda3\\envs\\hg_pl\\lib\\site-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): started\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): finished with status 'error'\n",
      "Failed to build llama-cpp-python\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Ã— Building wheel for llama-cpp-python (pyproject.toml) did not run successfully.\n",
      "  â”‚ exit code: 1\n",
      "  â•°â”€> [20 lines of output]\n",
      "      \u001b[32m*** \u001b[1mscikit-build-core 0.11.6\u001b[0m using \u001b[34mCMake 4.1.0\u001b[39m\u001b[0m \u001b[31m(wheel)\u001b[0m\n",
      "      \u001b[32m***\u001b[0m \u001b[1mConfiguring CMake...\u001b[0m\n",
      "      2025-08-28 15:30:15,114 - scikit_build_core - WARNING - Can't find a Python library, got libdir=None, ldlibrary=None, multiarch=None, masd=None\n",
      "      loading initial cache file C:\\Users\\venus\\AppData\\Local\\Temp\\tmpf3an4j0b\\build\\CMakeInit.txt\n",
      "      -- Building for: NMake Makefiles\n",
      "      \u001b[31mCMake Error at CMakeLists.txt:3 (project):\n",
      "        Running\n",
      "      \n",
      "         'nmake' '-?'\n",
      "      \n",
      "        failed with:\n",
      "      \n",
      "         no such file or directory\n",
      "      \n",
      "      \u001b[0m\n",
      "      \u001b[0mCMake Error: CMAKE_C_COMPILER not set, after EnableLanguage\u001b[0m\n",
      "      \u001b[0mCMake Error: CMAKE_CXX_COMPILER not set, after EnableLanguage\u001b[0m\n",
      "      -- Configuring incomplete, errors occurred!\n",
      "      \u001b[31m\n",
      "      \u001b[1m***\u001b[0m \u001b[31mCMake configuration failed\u001b[0m\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for llama-cpp-python\n",
      "ERROR: Failed to build installable wheels for some pyproject.toml based projects (llama-cpp-python)\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y llama-cpp-python llama-cpp-python-cu121 llama-cpp-python-cu118\n",
    "!pip install llama-cpp-python\n",
    "# GTX ì„ ìœ„í•˜ì—¬... ã… ã… "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b52c5cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\venus\\.cache\\huggingface\\hub\\models--Qwen--Qwen2.5-0.5B-Instruct-GGUF\\snapshots\\9217f5db79a29953eb74d5343926648285ec7e67\\qwen2.5-0.5b-instruct-q4_k_m.gguf\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "model_path = hf_hub_download(repo_id=\"Qwen/Qwen2.5-0.5B-Instruct-GGUF\", filename = \"qwen2.5-0.5b-instruct-q4_k_m.gguf\") # ë‹¤ìš´ë¡œë“œ íŒŒì¼ì˜ local path ì €ì¥\n",
    "print(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75ff432d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "from huggingface_hub import hf_hub_download\n",
    "import os\n",
    "\n",
    "# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ & ë¡œë“œ\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"Qwen/Qwen2.5-0.5B-Instruct-GGUF\",\n",
    "    filename=\"qwen2.5-0.5b-instruct-q4_k_m.gguf\"\n",
    ")\n",
    "\n",
    "llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_ctx=2048,     \n",
    "    n_threads= max(4, os.cpu_count() - 2),\n",
    "    n_batch=256,    \n",
    "    n_gpu_layers=0,  # CPU ì „ìš© GTX 1050...\n",
    "    verbose=False      # â† ë¡œê·¸ ìˆ¨ê¹€\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "60e29efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====== ğŸ“– í˜„ì¬ê¹Œì§€ì˜ ì´ì•¼ê¸° (í†µí•© ì¶œë ¥) ======\n",
      "\n",
      "[1] ìƒ›ë³„ì´ë¼ëŠ” ê³ ì–‘ì´ëŠ” ë§ˆë²•ì‚¬ê°€ í‚¤ìš°ëŠ” ê³ ì–‘ì´ì˜€ë‹¤. ì–´ëŠë‚  ë§ˆë²•ì‚¬ê°€ ë§í–ˆë‹¤. ìƒ›ë³„ì•„ ì ì‹œë™ì•ˆ ë‚˜ëŠ” ì§‘ì„ ë– ë‚˜ê²Œ ë˜ì—ˆë‹¨ë‹¤. ê·¸ë™ì•ˆ ì§‘ì„ ì§€ì¼œì¤„ ìˆ˜ ìˆê² ë‹ˆ? ìƒ›ë³„ì´ëŠ” ì•„ë¬´ê²ƒë„ ëª¨ë¥¸ì±„ ì•¼ì˜¹ì´ë¼ê³  ëŒ€ë‹µí–ˆë‹¤. ê·¸ë•Œê¹Œì§€ ëª¨ë¥´ê³  ìˆì—ˆë‹¤. ìƒ›ë³„ì´ì˜ ì£¼ì¸ì´ ëŒì•„ì˜¤ì§€ ì•Šì„ ê²ƒì´ë€ ì‚¬ì‹¤ì„... ìƒ›ë³„ì´ëŠ” í•˜ë£¨ê°€ ì§€ë‚˜ê³  ì´í‹€ì´ ì§€ë‚˜ê³  ì–¸ì  ê°„ ì£¼ì¸ì´ ëŒì•„ì˜¬ ê²ƒì´ë¼ ë¯¿ìœ¼ë©° ê¼¬ë°• 1ë…„ì„ ê¸°ë‹¤ë ¸ë‹¤. ê·¸ëŸ¬ë‚˜ ì£¼ì¸ì€ ëŒì•„ì˜¤ì§€ ì•Šì•˜ë‹¤. ì£¼ì¸ì´ ìì‹ ì„ ë²„ë ¸ìœ¼ë¦¬ë¼ ìƒê°í•˜ì§€ ì•ŠëŠ” ìˆœìˆ˜í•œ ê³ ì–‘ì´ì¸ ìƒ›ë³„ì€ ì£¼ì¸ì´ ê¸¸ì„ ìƒì–´ë²„ë ¸ìœ¼ë¦¬ë¼ ìƒê°í–ˆë‹¤. ê·¸ë˜ì„œ ìƒ›ë³„ì´ëŠ” ì§‘ì„ ë‚˜ì„°ë‹¤. ì£¼ì¸ì„ ì°¾ê¸°ìœ„í•´, ì£¼ì¸ì„ ì°¾ì•„ì„œ ì§‘ìœ¼ë¡œ í•¨ê»˜ ëŒì•„ì˜¤ê¸° ìœ„í•´ ë§ì´ë‹¤. ìƒ›ë³„ì´ê°€ ì§‘ì„ ë‚˜ì„œê³  ì²˜ìŒìœ¼ë¡œ ë°©ë¬¸í•œ ê³³ì€ ì–¸ë•ì•„ë˜ ë§ˆì„ì´ì—ˆë‹¤. ì–¸ë•ì•„ë˜ ë§ˆì„ì€ ì‚¬ëŒë“¤ì´ ë¬´ë¦¬ì§€ì–´ ì‚´ê³  ìˆì—ˆëŠ”ë° ê·¸ê³³ì— ì‚¬ëŠ” ì‚¬ëŒë“¤ì€ ê³ ì–‘ì´ë¥¼ ì‹«ì–´í•˜ê¸° ë•Œë¬¸ì— ì£¼ì¸ì´ ê°€ì§€ë§ë¼ê³  ë‹¹ë¶€í–ˆì—ˆë‹¤. ê·¸ëŸ¬ë‚˜ ìƒ›ë³„ì´ëŠ” ì£¼ì¸ì„ ì°¾ê¸° ìœ„í•´ ì‚¬ëŒì´ ì‚¬ëŠ” ë§ˆì„ë¡œ ê°€ì•¼ë§Œ í•œë‹¤ê³  ìƒê°í–ˆë‹¤. ë§ˆì„ì— ë„ì°©í•˜ì ì‚¬ëŒë“¤ì€ ìƒ›ë³„ì´ë¥¼ ë°”ë¼ë³´ë©° ê¸°ë¶„ë‚˜ì˜ë‹¤ëŠ” í‘œì •ì„ ì§€ì—ˆë‹¤. ìˆœìˆ˜í•œ ìƒ›ë³„ì´ëŠ” ê´œì°®ìœ¼ë¦¬ë¼ ìƒê°í•˜ë©° ê±¸ì–´ë‚˜ê°”ë‹¤. ê·¸ë•Œ ë§ˆì„ì˜ í•œ ì‚¬ëŒì´ ë©€ë¦¬ì„œ ìƒ›ë³„ì´ì—ê²Œ ëŒì„ ë˜ì¡Œê³ , ì•„ë¬´ê²ƒë„ ëª¨ë¥´ëŠ” ìƒ›ë³„ì´ê°€ ëŒì„ ë§ìœ¼ë ¤ëŠ” ìˆœê°„...! ì˜†ì—ì„œ ëˆ„êµ°ê°€ ëŒì„ ë§‰ì•„ì£¼ì—ˆë‹¤. ìƒ›ë³„ì´ê°€ ë†€ë€ ëˆˆìœ¼ë¡œ ì˜¬ë ¤ë‹¤ë³´ì ê·¸ê³³ì—” ë¹¨ê°„ë¨¸ë¦¬ë¥¼ ê°€ì§„ í•œ ì†Œë…€ê°€ ìˆì—ˆë‹¤.\n",
      "\n",
      "[2] [ì œì•½] í•œ ë¬¸ë‹¨ë§Œ, ì¤„ë°”ê¿ˆ 0~1íšŒ, ë°˜ë³µ í‘œí˜„ ìì œ\n",
      "\n",
      "[3] ë‚˜ëŠ” ê·¸ ì†Œë…€ì—ê²Œ ë§í–ˆë‹¤. ë‹¹ì‹ ì´ ê³ ì–‘ì´ë¥¼ ì‹«ì–´í•˜ëŠ” ì´ìœ ê°€ ë¬´ì—‡ì¼ê¹Œ? ë‹¹ì‹ ì€ ê³ ì–‘ì´ëŠ” ì‹¬ì§€ì–´ ê°€ì¡±ê³¼ë„ ì‚¬ë‘í•˜ì§€ ì•ŠëŠ” ë¶„ë“¤ì´ ìˆë‹¤ëŠ” ê²ƒì„ ì•Œê³  ìˆëŠ” ê±°ë‹¤. í•˜ì§€ë§Œ ì´ë“¤ì€ ê³ ì–‘ì´ë¥¼ ì‚¬ë‘í•˜ëŠ” ì‚¬ëŒë“¤ê³¼ ê°™ì€ ì‚¬ëŒë“¤ì„ ìœ„í•´ ì‹¸ìš°ëŠ” ê±°ì•¼. ê³ ì–‘ì´ì˜ ì†ì„ ì¡ëŠ” ê²ƒì€ ê·¸ì € ê³ ì–‘ì´ê°€ ì‹«ì–´í•˜ëŠ” ì‚¬ëŒë“¤ì—ê²Œë§Œ ê°€ëŠ¥í•˜ë‹¤ëŠ” ê±°ì•¼. ê³ ì–‘ì´ëŠ” ì‹¬ì§€ì–´ ê°€ì¡±ê³¼ë„ ì‚¬ë‘í•˜ì§€ ì•ŠëŠ” ì‚¬ëŒë“¤ì„ ìœ„í•´ ì‹¸ìš°ëŠ” ê±°ì•¼. ê·¸ë“¤ì€ ê³ ì–‘ì´ë¥¼ ì‚¬ë‘í•˜ëŠ” ì‚¬ëŒë“¤ê³¼ ê°™ì€ ì‚¬ëŒë“¤ì„ ìœ„í•´\n",
      "\n",
      "\n",
      "âœ… ì €ì¥ ì™„ë£Œ: C:\\NLP\\practing\\story.json\n",
      "âœ… story.txt ì €ì¥ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "from huggingface_hub import hf_hub_download\n",
    "import os, json\n",
    "from pathlib import Path\n",
    "\n",
    "SAVE_PATH = Path(\"story.json\")\n",
    "\n",
    "def load_story():\n",
    "    if SAVE_PATH.exists():\n",
    "        with open(SAVE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "    return []\n",
    "\n",
    "def save_txt(story, txt_path=\"story.txt\"):\n",
    "    with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"ì†Œì„¤ ì´ˆì•ˆ\\n\\n\")\n",
    "        for i, p in enumerate(story, 1):\n",
    "            f.write(f\"[{i}] {p}\\n\\n\")\n",
    "\n",
    "def save_story(story):\n",
    "    with open(SAVE_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(story, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def continue_paragraph(paragraphs, genre=\"íŒíƒ€ì§€ ë¯¸ìŠ¤í„°ë¦¬\", style=\"ê°ì„± ë¬˜ì‚¬\"):\n",
    "    guide = (\n",
    "        f\"[ì—­í• ] í•œêµ­ì–´ ë¬¸í•™ ì†Œì„¤ê°€\\n\"\n",
    "        f\"[ì¥ë¥´] {genre}\\n\"\n",
    "        f\"[ìŠ¤íƒ€ì¼] {style} / ë¬¸ì¥ì€ ìì—°ìŠ¤ëŸ½ê³  ì—°ê²°ì„± ìˆê²Œ, ê³¼ì¥ëœ ì¶”ìƒ í‘œí˜„ ê¸ˆì§€\\n\"\n",
    "        f\"[ì œì•½] í•œ ë¬¸ë‹¨ë§Œ, ì¤„ë°”ê¿ˆ 0~1íšŒ, ë°˜ë³µ í‘œí˜„ ìì œ\\n\"\n",
    "    )\n",
    "    sofar = \"\\n\\n\".join(paragraphs)\n",
    "    prompt = (\n",
    "        f\"{guide}\\n\"\n",
    "        f\"ì§€ê¸ˆê¹Œì§€ì˜ ì´ì•¼ê¸°:\\n{sofar}\\n\\n\"\n",
    "        f\"[ë‹¤ìŒ ë¬¸ë‹¨]\\n\"\n",
    "    )\n",
    "    out = llm(\n",
    "        prompt,\n",
    "        max_tokens=140,\n",
    "        temperature=0.6,\n",
    "        top_p=0.8,\n",
    "        top_k=40,\n",
    "        repeat_penalty=1.07,\n",
    "        stop=[\"\\n\\n\\n\",\"###\",\"</s>\"]\n",
    "    )\n",
    "    text = out[\"choices\"][0][\"text\"].strip()\n",
    "    return text.split(\"\\n\\n\")[0].strip()\n",
    "\n",
    "story = load_story()\n",
    "if not story:\n",
    "    start = input(\"ì†Œì„¤ ì²« ë¬¸ì¥ì„ ì…ë ¥í•˜ì„¸ìš”: \")\n",
    "    story = [start]\n",
    "\n",
    "turns = int(input(\"ì´ë²ˆì— ëª‡ ë¬¸ë‹¨ì„ ë” ì´ì–´ ì“¸ê¹Œìš”? (ì˜ˆ: 2): \") or \"2\")\n",
    "for _ in range(turns):\n",
    "    story.append(continue_paragraph(story))\n",
    "\n",
    "print(\"\\n====== ğŸ“– í˜„ì¬ê¹Œì§€ì˜ ì´ì•¼ê¸° (í†µí•© ì¶œë ¥) ======\\n\")\n",
    "for i, para in enumerate(story, 1):\n",
    "    print(f\"[{i}] {para}\\n\")\n",
    "\n",
    "# ì €ì¥\n",
    "save_story(story)\n",
    "print(f\"\\nâœ… ì €ì¥ ì™„ë£Œ: {SAVE_PATH.resolve()}\")\n",
    "\n",
    "save_txt(story)\n",
    "print(\"âœ… story.txt ì €ì¥ ì™„ë£Œ\")\n",
    "# llm(prompt, ... )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hg_pl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
